# Linear Models, and beyond

In reality, rm-ANOVA and LMEMs are just _special cases_ of broader model classes (General Linear Models and Generalized Linear Mixed Models, respectively). Understanding this is critical to fully capture the constraints of such models and to understand how GAMs overcome those limitations. This section will briefly present the different Statistical model classes, indicate how rm-ANOVA and LMEMs fit within that framework and introduce the theory of GAMs.


## Linear Models (LMs) and Linear Mixed Models (LMEMs)

Linear models (LMs) are those that assume a normal (Gaussian) distribution of the response, such as rm-ANOVA. These are by far the models most commonly used to analyze data within the biomedical research community, and only incorporate _fixed effects_. On the other hand, Linear Mixed Effect Models (LMEMs) are those that assume a normally-distributed response but also incorporate _random effects_, as it has been described in Section \@ref(LMEM-case).

In matrix notation, a LM has the general form:

\begin{equation}
\textbf{y} = \textbf{X} \boldsymbol{\beta}
(\#eq:LM-matrix)
\end{equation}

Where $\textbf{y}$ a vector of observations, $\textbf{X}$ is a matrix that contains the measured values of the covariates, and $\boldsymbol{\beta}$ is a vector of the parameters to be estimated. The "model equation" form of Equation \@ref(eq:LM-matrix) for a rm-ANOVA model of two treatment groups corresponds to Equation \@ref(eq:linear-model). It is important to remember that the error term $\varepsilon_{ijt}$ in Equation \@ref(eq:linear-model) is due to random sampling from the normal distribution.

## Generalized Linear Models (GLMs)

A major limitation of LMs is their assumption of normality. If the data is non-normal, a transformation is necessary in order to properly fit the model. However, transformation lead to poor model performance [@ohara2010], and can cause problems with the biological interpretation of the model estimates. McCullagh and Nelder [@nelder1972] defined General Linear Models (GLMs) as an extension of LMs, where the data does not need to be normally distributed. To achieve this, consider the following:

\begin{equation}
y_{ijt} \sim \mathcal{D}(\mu_{ijt},\phi)
(\#eq:GLM-y)
\end{equation}

Where $y_{ijt}$ is the observation $i$ in group $j$ at time $t$, which is assumed to come randomly from some distribution of the exponential family $\mathcal{D}$, with some mean $\mu_{ijt}$ and a dispersion parameter $\phi$ (which in the Gaussian case is the variance $\sigma^{2}$). The mean $\mu_{ijt}$ is also known as the _expected value_ (also called _expectation_) ($E$) of the observed response. Then, the _linear predictor_ $\eta$, which defines the relationship between the mean and the covariates can be defined as

\begin{equation}
\eta_{ijt}=\beta_0+\beta_{j}x_{ijt}
(\#eq:GLM-eta)
\end{equation}

Where $\eta_{ijt}$ is the linear predictor for each observation, $\beta_0$ is the intercept, and $\beta_{j}$ is the model parameter for each group; and $x_{ijt}$ represents the covariates from each group at each time point.

Finally, 

\begin{equation}
E(y_{ijt})=\mu_{ijt}=g^{-1}(\eta_{ijt})
(\#eq:GLM-Expectation)
\end{equation}
Where $E(y_{ijt})$ is the expectation, $g^-{1}$ is the inverse of the _link function_  $g$, which transforms the values from the response scale to the scale of the linear predictor $\eta$ (Equation \@ref(eq:GLM-eta)). Therefore, LMs (such as rm-ANOVA) are a special case of GLMs where the response is normally distributed.

## Generalized linear mixed models (GLMMs)

Although GLMs relax the normality assumption, they can only accommodate fixed effects. Generalized Linear Mixed Models (GLMMs) are an extension of GLMs that incorporate _random effects_, which have an associated probability distribution [@mcculloch2001]. Therefore, in GLMMs the _linear predictor_ takes the form

\begin{equation}
\eta_{ijt}=\beta_0+\beta_{j}x_{ijt}+\mu_{ij}
(\#eq:GLMM-eta)
\end{equation}

Where $\mu_{ij}$ corresponds to the random effects that can be estimated within each subject in each group, and all the other symbols correspond to the notation of the previous equations. Therefore, LMEMs are special case of GLMMs where the distribution of the response is normally distributed [@nelder1972]. In-depth and excellent discussions about LMs, GLMs and GLMMs can be found in refs [@dobson2008] and [@stroup2013].

### GAMs as a special case of Generalized Linear Models{#GAM-theory}

#### GAMs and Basis Functions

Generalized additive models (GAMs) are a family of regression-based methods for estimating smoothly varying trends and are an extension of the GLM family [@simpson2018;@wood2017;@hastie1987]. A GAM version of a linear model can be written as:


\begin{equation}
  y_{ijt}=\beta_0+f(x_t\mid \beta_j)+\varepsilon_{ijt}
  (\#eq:GAM)
\end{equation}

Where $y_{ijt}$ is the response at time $t$ of subject  $i$ in group $j$, $\beta_0$ is the expected value at time 0, and where the change of $y_{ijt}$ over time instead of being defined by $\beta_{j}x_{ijt}$ as in Equation \@ref(eq:GLM-eta) is represented by the _smooth function_ $f(x_t\mid \beta_j)$ with inputs as the covariates $x_t$ and parameters $\beta_j$, and $\varepsilon_{ijt}$ represents the residual. 

In contrast to the linear functions used to model the relationship between the covariates and the response in rm-ANOVA or LMEM, GAMs use more flexible _smooth functions_. This approach is advantageous as it does not restrict the model to a linear relationship, although a GAM can estimate a linear relationship if the data is consistent with a linear response. One possible set of functions for $f(x_t\mid \beta_j)$ that allow for non-linear responses are polynomials (which can be used in LMEMs), but a major limitation is that polynomials create a "global" fit as they assume that the same relationship exists everywhere, which can cause problems with inference [@beck1998]. In particular, polynomial fits are known to show boundary effects because as $t$ goes to $\pm \infty$, $f(x_t \mid \beta_j)$ goes to $\pm \infty$ which is almost always unrealistic and causes bias at the endpoints of the time period.

The smooth functional relationship between the covariates and the response in GAMs is specified   using a semi-parametric relationship that can be fit within the GLM framework, by using _basis function_ expansions of the covariates and by estimating random coefficients associated with these basis functions. A _basis_ is a set of functions that spans the mathematical space within which the true but unknown $f(x_t\mid \beta_j)$  is thought to exist [@simpson2018]. For the linear model in Equation \@ref(eq:linear-model), the basis coefficients are $\beta_1$, $\beta_2$ and $\beta_3$ and the basis vectors are $time_t$, $treatment_j$ and $time_t \times treatment_j$. The basis function then, is the combination of basis coefficients and basis vectors that map the possible relationship between the covariates and the response [@hefley2017], which in the case of Equation \@ref(eq:linear-model) is restricted to a linear family of functions.  In the case of Equation \@ref(eq:GAM), the basis functions are contained in the expression $f(x_t\mid \beta_j)$, which means that the model allows for non-linear relationships among the covariates.

Splines (which derive their name from the physical devices used by draughtsmen to draw smooth curves) are commonly used as _basis functions_, as they have a long history in solving semi-parametric statistical problems and are often a default choice to fit GAMs as they are a simple, flexible and powerful option to obtain smoothness [@wegman1983]. Although different types of splines exist, cubic, thin plate splines, and thin plate regression splines will be briefly discussed next to give a general idea of these type of basis functions, and their use within the GAM framework. 

Cubic splines (CS), are smooth curves constructed from cubic polynomials joined together in a manner that enforces smoothness. The use of CS as smoothers in GAMs was discussed within the original GAM framework [@hastie1987], but they are limited by the fact that their implementation requires the selection of some points along the covariates (known as 'knots', the points where the bending of the smooth will occur) to obtain the reduced basis, which could affect the model fit [@wood2003]. A solution to the "knot" placement of CS is provided by thin plate splines (TPS), which provide optimal smooth estimation without knot placement, but that are computationally costly to calculate [@wood2003; @wood2017].

In contrast, thin regression splines (TPRS) provide a reasonable "low rank" (truncated) approximation to the optimal TPS estimation, which can be implemented in an efficient computational manner [@wood2003]. Like TPS, TPRS only require the number of basis to be used to create the smoother (for mathematical details on both TPS and TPRS see refs. [@wood2003] and [@wood2017]).

To further clarify the concept of basis functions and smooth functions, consider the simulated response for Group 1 in Figure \@ref(fig:l-q-response)C. The simplest GAM model that can be used to estimate such response is that of a single smooth term for the time effect; i.e., a model that fits a smooth to the trend of the group through time. A computational requisite is that the number of basis functions to be used to create the smooth cannot be larger than the number of unique values from the independent variable. Because the data has six unique time points, we can specify a maximum of six basis functions (including the intercept) to create the smooth (it is important to note that is not necessary to specify an equal number of basis to the number of unique values in the independent variable; less basis functions can be specified to create the smooth as well, as long as they reasonably capture the trend of the data).

If five basis functions are used to fit a GAM for the data that appears in Figure \@ref(fig:l-q-response)C, the resulting fitting process is shown in Figure \@ref(fig:basis-plot)A. The four basis functions (and the intercept) are shown. Each of the basis functions is composed of six different points (because there are six points on the timeline). To control the "wiggliness" of the fit, each of the basis functions of Figure \@ref(fig:basis-plot)A is weighted by multiplying it by a coefficient according to the matrix of Figure \@ref(fig:basis-plot)B. The parameter estimates are penalized (shrunk towards 0) where the penalty reduces the "wiggliness" of the smooth fit to prevent overfitting. A weak penalty estimate will result in wiggly functions whereas a strong penalty estimate provides evidence that a linear response is appropriate.


```{r,basis-functions-plot, echo=FALSE,include=FALSE,message=FALSE,warning=FALSE}
n_time = 6
 x <- seq(1,6, length.out = n_time)
 mu <- matrix(0, length(x), 2)
 mu[, 1] <-  -(0.25 * x^2) +1.5*x-1.25 #mean response
 mu[, 2] <- (0.25 * x^2) -1.5*x+1.25 #mean response
 y <- array(0, dim = c(length(x), 2, 10))
 errors <- array(0, dim = c(length(x), 2, 10))
 for (i in 1:2) {     # number of treatments
     for (j in 1:10) {  # number of subjects
         # compound symmetry errors
         errors[, i, j] <- rmvn(1, rep(0, length(x)), 0.1 * diag(6) + 0.25 * matrix(1, 6, 6))
         y[, i, j] <- mu[, i] + errors[, i, j]
     }
 }
 
 #label each table
  dimnames(y) <- list(time = x, treatment = 1:2, subject = 1:10)
 dimnames(errors) <- list(time = x, treatment = 1:2, subject = 1:10)
 dimnames(mu) <- list(time = x, treatment = 1:2)
 
 #Convert to dataframes with subject, time and group columns
 dat <- as.data.frame.table(y, responseName = "y")
 dat_errors <- as.data.frame.table(errors, responseName = "errors")
 dat_mu <- as.data.frame.table(mu, responseName = "mu")
 dat <- left_join(dat, dat_errors, by = c("time", "treatment", "subject"))
 dat <- left_join(dat, dat_mu, by = c("time", "treatment"))
 dat$time <- as.numeric(as.character(dat$time))
 
 #label subject per group
 dat <- dat %>%
     mutate(subject = factor(paste(subject, treatment, sep = "-")))
  
 #extract  "Group 1" to fit the GAM
  dat<-subset(dat,treatment==1)
 #keep just the response and timepoint columns
   dat<-dat[,c('y','time')]

   #GAM model of time, 5 basis functions
gm<-gam(y~s(time,k=5),data=dat)

#model_matrix (also known as) 'design matrix'
#will contain the smooths used to create  model 'gm'
model_matrix<-as.data.frame(predict(gm,type='lpmatrix'))


time<-c(1:6)

basis<-model_matrix[1:6,] #extracting basis (because the values are repeated after every 6 rows)
#basis<-model_matrix[1:6,-1] #extracting basis
colnames(basis)[colnames(basis)=="(Intercept)"]<-"s(time).0"
basis<-basis %>% #pivoting to long format
  pivot_longer(
    cols=starts_with("s")
  )%>%
  arrange(name) #ordering

#length of dataframe to be created: number of basis by number of timepoints (minus 1 for the intercept that we won't plot)
ln<-6*(length(coef(gm))) 

basis_plot<-data.frame(Basis=integer(ln),
                       value_orig=double(ln),
                       time=integer(ln),
                       cof=double(ln)
)

basis_plot$time<-rep(time) #pasting timepoints
basis_plot$Basis<-factor(rep(c(1:5),each=6)) #pasting basis number values
basis_plot$value_orig<-basis$value #pasting basis values
basis_plot$cof<-rep(coef(gm)[1:5],each=6) #pasting coefficients
basis_plot<-basis_plot%>%
  mutate(mod_val=value_orig*cof) #the create the predicted values the bases need to be 
#multiplied by the coefficients

#creating labeller to change the labels in the basis plots

basis_names<-c(
  `1`="Intercept",
  `2`="1",
  `3`="2",
  `4`="3",
  `5`="4"
)

#calculating the final smooth by aggregating the basis functions

smooth<-basis_plot%>% 
  group_by(time)%>%
  summarize(smooth=sum(mod_val))


#original basis
sz<-1
p11<-ggplot(basis_plot,
            aes(x=time,
                y=value_orig,
                colour=as.factor(Basis)
                )
            )+
  geom_line(size=sz,
            show.legend=FALSE)+
  geom_point(size=sz+1,
             show.legend = FALSE)+
  labs(y='Basis functions')+
  facet_wrap(~Basis,
             labeller = as_labeller(basis_names)
             )+
  theme_classic()+
  thm1
  

#penalized basis
p12<-ggplot(basis_plot,
            aes(x=time,
                y=mod_val,
                colour=as.factor(Basis)
                )
            )+
  geom_line(show.legend = FALSE,
            size=sz)+
  geom_point(show.legend = FALSE,
             size=sz+1)+
  labs(y='Penalized \n basis functions')+
  scale_y_continuous(breaks=seq(-1,1,1))+
  facet_wrap(~Basis,
             labeller=as_labeller(basis_names)
             )+
  theme_classic()+
  thm1

#heatmap of the  coefficients
x_labels<-c("Intercept","1","2","3","4")
p13<-ggplot(basis_plot,
            aes(x=Basis,
                y=Basis))+
  geom_tile(aes(fill = cof), 
            colour = "black") +
    scale_fill_gradient(low = "white",
                        high = "#B50A2AFF")+ 
  labs(x='Basis',
       y='Basis')+
  scale_x_discrete(labels=x_labels)+
  geom_text(aes(label=round(cof,2)),
            size=7,
            show.legend = FALSE)+
  theme_classic()+
  theme(legend.title = element_blank())
  
#plotting simulated datapoints and smooth term
p14<-ggplot(data=dat,
            aes(x=time,y=y))+
  geom_point(size=sz+1,alpha=0.5)+
  thm1+
  labs(y='Simulated \n response')+
  geom_line(data=smooth,
            aes(x=time,
                y=smooth),
            color="#6C581DFF",
            size=sz+1)+
  theme_classic()
  

#Combining all
b_plot<-p11+p13+p12+p14+plot_annotation(tag_levels='A')&
  theme(
     text=element_text(size=18)
     )


```

To get the weighted basis functions, each basis (from Figure Figure \@ref(fig:basis-plot)A) is multiplied by the corresponding coefficients in Figure \@ref(fig:basis-plot)B, thereby increasing or decreasing the original basis functions. Figure \@ref(fig:basis-plot)C shows the resulting weighted basis functions. Note that the magnitude of the weighting for the first basis function has resulted in a decrease of its overall contribution to the smoother term (because the coefficient for that basis function is negative and less than 1). On the other hand, the third basis function has roughly doubled its contribution to the smooth term. Finally, the weighted basis functions are added at each timepoint to produce the smooth term. The resulting smooth term for the effect of _time_ is shown in Figure \@ref(fig:basis-plot)D (orange line), along the simulated values per group, which appear as points.

(ref:basis-plot-caption) Basis functions for a single smoother for time. A: Basis functions for a single smoother for time for the simulated data of Group 1 from Figure 2. B: Matrix of basis function weights. Each basis function is multiplied by a coefficient which can be positive or negative. The coefficient determines the overall effect of each basis in the final smoother. C: Weighted basis functions. Each of the four basis functions of panel A has been weighted by the corresponding coefficient shown in Panel B. Note the corresponding increase (or decrease) in magnitude of each weighted basis function. D: Smoother for time and original data points. The smoother (line) is the result of the sum of each weighted basis function at each time point, with simulated values for the group shown as points.

```{r,basis-plot,fig.width=10, fig.height=10, out.width='75%', fig.align='center',echo=FALSE,message=FALSE, fig.show='hold', fig.cap='(ref:basis-plot-caption)'}

par(mar = c(2, 2, 2, 2))

b_plot

```

### A Bayesian interpretation of GAMs

Bayes' theorem states that the probability of an event can be calculated using prior knowledge or belief [@mcelreath2018]. In the case of data that shows non-linear trends, the belief that the _true_ trend of the data is likely to be smooth rather than extremely "wiggly" introduces the concept of a prior distribution for wiggliness (and therefore a Bayesian view) of GAMs [@wood2017]. Moreover, GAMs are considered "empirical" Bayesian models when fitted using the package _mgcv_ because the smoothing parameters are estimated from the data (and not from a prior distribution as in the "fully Bayesian" case, which can be fitted using JAGS, Stan, or other probabilistic programming language) [@miller2019]. Therefore, the confidence intervals (CIs) calculated for the smooth terms using _mgcv_ are considered empirical Bayesian posterior credible intervals [@pedersen2019], which have good "frequentist" coverage (point-wise coverage or "single point" coverage), and _across the function_ coverage [@wood2017].

To understand this last part, it is worth reminding that a CI provides an estimate of the region where the “true” or “mean” value of a function exists, taking into account the randomness introduced by the sampling process. Because random samples from the population are used to calculate the "true" value of the function, there is inherent variability in the estimation process and the CI provides a region with a nominal value (usually, 95%) where the function is expected to lie. In a "point-wise" CI for non-linear data, the "true" function will lie outside of the CI in certain regions at a given frequency (because the data does not follow a linear trend). This means that if a point-wise CI is obtained for 100 random samples, the total number of CIs that contain the true function _entirely_ is much less than the nominal value of the CI (95%, usually).

Contrary to this, an _across the function_ CI (like those estimated for GAMs using _mgcv_) contains the true function through the entire range of the covariates (which would be time in the case of longitudinal data). For GAMs this means that if repeated samples are taken and a GAM and corresponding CIs are calculated, the percentage of CIs entirely containing the true function over the entire time period would be close to the nominal value (i.e., if 100 random samples are obtained and a GAM and CI is calculated for each one of them, it would be expected that 95 out of the 100 fitted CIs entirely contain the true function), thereby allowing more robust estimates from the model. In-depth theory of the Bayesian interpretation of GAMs is beyond the scope of this paper, but can be found in [@miller2019;@wood2017;@simpson2018] and [@marra2012]. With this brief introduction to the Bayesian interpretation of GAMs, we henceforth refer to the confidence intervals for the smooths in GAMs as "empirical Bayesian" through the rest of this paper.
