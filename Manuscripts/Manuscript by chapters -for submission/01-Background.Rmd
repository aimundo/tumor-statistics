# Background

Longitudinal studies are designed to repeatedly measure a variable of interest in a group (or groups) of subjects, with the intention of observing the evolution of effect across time rather than analyzing a single time point (e.g., a cross-sectional study). Biomedical research frequently uses longitudinal studies to analyze the evolution of a "treatment" effect across multiple time points; and in such studies the subjects of analysis range from animals (mice, rats, rabbits), to human patients, cells, or blood samples, among many others. Tumor response [@roblyer2011;@tank2020;@pavlov2018;@demidov2018], antibody expression [@ritter2001] [@roth2017], and cell metabolism [@jones2018] [@skala2010] are examples of the different situations where researchers have used longitudinal designs to study some  physiological response. Because the frequency of the measurements in a longitudinal study is dependent on the biological phenomena of interest and the experimental design of the study, the frequency of such measurements can range from minute intervals to study a short-term response such as anesthesia effects in animals[@greening2018], to weekly measurements to analyze a mid-term response like the evolution of dermatitis symptoms in breast cancer patients [@sio2016], to monthly measurements to study a long-term response such as mouth opening following radiotherapy (RT) in neck cancer patients [@kamstra2015]. 


Traditionally, a “frequentist” or "classical" statistical paradigm is used in biomedical research to derive inferences from a longitudinal study. The frequentist paradigm regards probability as the limit of the expected outcome when an experiment is repeated a large number of times [@wagenmakers2008], and such view is applied to the analysis of longitudinal data by assuming a null hypothesis under a statistical model that is often an _analysis of variance over repeated measures_ (repeated measures ANOVA or rm-ANOVA). The rm-ANOVA model makes three key assumptions regarding longitudinal data: 1) linearity of the response across time, 2) constant correlation across same-subject measurements, and 3) observations from each subject are obtained at all time points through the study (a condition also known as _complete observations_) [@gueorguieva2004] [@schober2018]. 

The expected linear behavior of the response through time is a key requisite in rm-ANOVA [@pinheiro2006]. This "linearity assumption" in rm-ANOVA implies that the model is misspecified when the data does not follow a linear trend, which results in unreliable inference. In biomedical research, non-linear trends are the norm rather than the exception in longitudinal studies. A particular example of this non-linear behavior in longitudinal data arises in measurements of tumor response to chemo and/or radiotherapy in preclinical and clinical settings [@roblyer2011] [@skala2010] [@vishwanath2009]. These studies have shown that the collected signal does not follow a linear trend over time, and presents extreme variability at different time points, making the fit of rm-ANOVA model inconsistent with the observed variation. Therefore, when rm-ANOVA is used to draw inference of such data the estimates are inevitably biased, because the model is only able to accommodate linear trends that fail to adequately represent the biological phenomenon of interest.


A _post hoc_ analysis is often used in conjunction with rm-ANOVA to perform repeated comparisons to estimate a _p-value_, which in turn is used as a measure of significance.
Although it is possible that a _post hoc_ analysis of rm-ANOVA is able to find “significant” _p-values_( _p_<0.05) from non-linear data, the validity of such metric is dependent on how adequate the model fits the data. In other words, _p-values_ are valid only if the model and the data have good agreement; if that is not the case, a "Type III" error (known as "model misspecification") occurs[@dennis2019]. For example, model misspecification will occur when a model that is only able to explain linear responses (such as rm-ANOVA) is fitted to data that follows a quadratic trend, thereby causing the resulting _p-values_ and parameter estimates to be invalid [@wang2019].

Additionally, the _p-value_ itself is highly variable, and multiple comparisons can inflate the false positivity rate (Type I error or $\alpha$) [@liu2010; @halsey2015], consequently biasing the conclusions of the study. Corrections exist to address the Type I error issue of multiple comparisons (such as Bonferroni [@abdi2010]), but they in turn reduce statistical power (1-$\beta$)[@nakagawa2004], and lead to increased Type II error (failing to reject the null hypothesis when the null hypothesis is false) [@gelman2012; @albers2019]. Therefore, the tradeoff of _post hoc_ comparisons in rm-ANOVA between Type I, II and III errors might be difficult to resolve in a biomedical longitudinal study where a delicate balance exists between statistical power and sample size.


On the other hand, the assumption of constant correlation in rm-ANOVA (often known as the _compound symmetry assumption_) is typically unreasonable because correlation between the measured responses often diminishes as the time interval between the observation increases [@ugrinowitsch2004].  Corrections can be made in rm-ANOVA in the absence of compound symmetry [@huynh1976; @greenhouse1959], but the effectiveness of the correction is limited by the size of the sample, the number of measurements[@haverkamp2017], and group sizes [@keselman2001]. In the case of biomedical research, where living subjects are frequently used, sample sizes are often not "large" due to ethical and budgetary reasons [@charan2013] which might cause the corrections for lack of compound symmetry to be ineffective.

Due to a variety of causes, the number of observations during a study can vary between all subjects. For example, in a clinical trial patients may voluntarily withdraw, whereas attrition  due to injury or weight loss in preclinical animal studies is possible. It is even plausible that unexpected complications with equipment or supplies arise that prevent the researcher from collecting measurements at certain time points. In each of these missing data scenarios, the _complete observations_ assumption of  classical rm-ANOVA is violated. When incomplete observations occur, a rm-ANOVA model is fit by excluding all subjects with missing observations from the analysis [@gueorguieva2004]. This elimination of partially missing data from the analysis can result in increased costs if the desired statistical power is not met with the remaining observations, because it would be necessary to enroll more subjects. At the same time, if the excluded observations contain insightful information that is not used, their elimination from the analysis may limit the demonstration of significant differences between groups. 

During the last decade, the biomedical community has started to recognize the limitations of rm-ANOVA in the analysis of longitudinal data. The recognition on the shortcomings of rm-ANOVA is exemplified by the  use of  linear mixed effects models (LMEMs) by certain groups to analyze longitudinal tumor response data [@skala2010;@vishwanath2009]. Briefly, LMEMs incorporate _fixed effects_, which correspond to the levels of experimental factors in the study (e.g., the different drug regimens in a clinical trial), and _random effects_, which account for random variation within the population (e.g., the individual-level differences not due to treatment such as weight or age). When compared to the traditional rm-ANOVA, LMEMs are more flexible as they can accommodate missing observations for multiple subjects and allow different modeling strategies for the variability within each measure in every subject [@pinheiro2006]. However, LMEMs impose restrictions in the distribution of the errors  of the random effects, which need to be normally distributed and independent [@gueorguieva2004;@barr2013]. And even more importantly, LMEMs also assume a linear relationship between the response and time [@pinheiro2006], making them unsuitable to analyze non-linear data.

As the rm-ANOVA and the more flexible LMEM approaches make overly restrictive assumptions regarding the linearity of the response, there is a need for biomedical researchers to explore the use of additional statistical tools that allow the data (and not an assumption in trend) to determine the trend of the fitted model, to enable appropriate inference. In this regard, generalized additive models (GAMs) present an alternative approach to analyze longitudinal data. Although not frequently used by the biomedical community, these semi-parametric models are customarily used in other fields to analyze longitudinal data.   Examples of the use of GAMs include the analysis of temporal variations in geochemical and palaeoecological data  [@rose2012; @pedersen2019; @simpson2018], health-environment interactions [@yang2012] and the dynamics of government in political science [@beck1998]. There are several advantages of GAMs over LMEMs and rm-ANOVA models: 1) GAMs can fit a more flexible class of smooth responses that enable the data to dictate the trend in the fit of the model, 2) they can model non-constant correlation between repeated measurements [@wood2017] and 3) can easily accommodate missing observations. Therefore, GAMs can provide a more flexible statistical approach to analyze non-linear biomedical longitudinal data than LMEMs and rm-ANOVA.

The current advances in programming languages designed for statistical analysis (specifically  $\textsf{R}$), have eased the computational implementation of traditional models such as rm-ANOVA and more complex approaches such as LMEMs and GAMs. In particular, $\textsf{R}$ [@r] has an extensive collection of documentation and functions to fit GAMs in the package _mgcv_ [@wood2016; @wood2017] that not only speed up the initial stages of the analysis but also enable the use of advanced modeling structures (e.g. hierarchical models, confidence interval comparisons) without requiring advanced programming skills from the user. At the same time, $\textsf{R}$ has many tools that simplify data simulation, an emerging strategy used to test statistical models [@haverkamp2017]. Data simulation methods allow the researcher to create and  explore different alternatives for analysis without collecting information in the field, reducing the time window between experiment design and its implementation, and simulation can be also used for power calculations and study design questions.   

This work provides biomedical researchers with a clear understanding of the theory and the practice of using GAMs to analyze longitudinal data using by focusing on four areas. First, the limitations of LMEMs and rm-ANOVA regarding linearity of response, constant correlation structures and missing observations are explained in detail. Second, the key theoretical elements of GAMs are presented using clear and simple mathematical notation while explaining the context and interpretation of the equations. Third, we illustrate the type of non-linear longitudinal data that often occurs in biomedical research using simulated data that reproduces patterns in previously reported studies [@vishwanath2009]. The simulated data experiments highlight the differences in inference between rm-ANOVA, LMEMs and GAMs on data similar to what is commonly observed in biomedical studies. Finally, reproducibility is emphasized by providing the code to generate the simulated data and the implementation of different models in $\textsf{R}$, in conjunction with a step-by-step guide demonstrating how to fit models of increasing complexity.  

In summary, this work will allow biomedical researchers to identify when the use of GAMs instead of rm-ANOVA or LMEMs is appropriate to analyze longitudinal data, and provide guidance on the implementation of these models to improve the standards for reproducibility  in biomedical research.
