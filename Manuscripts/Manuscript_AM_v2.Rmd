---
title: '**The statistical analysis of non-linear longitudinal data in biomedical research using generalized additive models**'
subtitle: _Beyond repeated measures ANOVA and Linear Mixed Models_
author:
- Ariel Mundo^[Department of Biomedical Engineering, University of Arkansas, Fayetteville]
- Timothy J. Muldoon^[Department of Biomedical Engineering, University of Arkansas,
  Fayetteville]
- John R. Tipton^[Department of Mathematical Sciences, University of Arkansas, Fayetteville]
output:
  bookdown::pdf_document2:
    keep_tex: yes
    fig_caption: yes
    extra_dependencies: ["subfig","breqn"]
  bookdown::word_document2: default
  bookdown::html_document2: default
  link-citations: yes
  css: style.css
  '': default
csl: Elsevier.csl
bibliography: refs.bib
---

# Background

A longitudinal study is designed to repeatedly measure a variable of interest in a group (or groups) of subjects, with the intention of observing the evolution of the treatment effect across time rather than analyzing a single time point (e.g., a cross-sectional study). Medical research commonly uses longitudinal studies to analyze the evolution of patients before, during and after treatment. For example, Sio et. al. analyzed the evolution of dermatitis symptoms in breast cancer patients after radiotherapy (RT), by taking weekly measurements over the course of two months [@sio2016], and Kamstra et al measured mouth opening at regular monthly intervals after RT in neck cancer patients [@kamstra2015]. Other studies using longitudinal designs in biomedical research have tumor  analyzed tumor response [@roblyer2011;@tank2020;@pavlov2018;@demidov2018], antibody expression [@ritter2001;@roth2017], and cell metabolism [@jones2018;@skala2010]. 


Traditionally, a “frequentist” or "classical" statistical paradigm is used in biomedical research to derive inferences from a longitudinal study. The frequentist paradigm regards probability as a limiting frequency[@wagenmakers2008] by assuming a null hypothesis under a statistical model that is often assumed to be an _analysis of variance over repeated measures_ (repeated measures ANOVA or rm-ANOVA). The rm-ANOVA model typically makes two key assumptions regarding longitudinal data: constant correlation across same-subject measurements, and observations from each subject are obtained at all time points through the study (a condition also known as _complete observations_) [@gueorguieva2004;@schober2018].

The assumption of constant correlation (often known as the _compoun symmetry assumption_) is typically unreasonable because correlation between the measured responses often diminishes as the time interval between the observation increases [@ugrinowitsch2004]. 
Due to a variety of causes, the number of observations during a study can vary between all subjects. For example, in a clinical trial voluntary withdrawal from one or multiple patients can occur, whereas attrition in preclinical animal studies due to injury or weight loss is possible. It is even plausible that unexpected complications with equipment or supplies arise that prevent the researcher from collecting measurements at certain time points. In each of these missing data scenarios,the _complete observations_ assumption of  classical rm-ANOVA is violated. 

When incomplete observations occur, a rm-ANOVA model is fit by excluding all subjects with missing observations from the analysis[@gueorguieva2004]. This elimination of partially missing data from the analysis can result in increased costs if the desired statistical power is not met with the remaining observations, because it would be necessary to enroll more subjects. At the same time, if the excluded observations contain insightful information that is not used, their elimination from the analysis may limit the demonstration of significant differences between groups. Additionally, rm-ANOVA uses a _post hoc_ analysis to assess differences between the measured response in different groups. A _post hoc_ analysis is based on multiple repeated comparisons to estimate a _p-value_, a metric that is widely used as a measure of significance. Because the _p-value_ is highly variable, multiple comparisons can inflate the false positivity rate [@liu2010;@halsey2015], consequently biasing the conclusions of the study. Another less known side effect of multiple comparisons is that they reduce statistical power ($\alpha$)[@nakagawa2004], which in turn can lead to fail to reject the null hypothesis although it is false (a type II error)[@albers2019].




During the last decade, the biomedical community has started to recognize the limitations of rm-ANOVA in the analysis of longitudinal information. The recognition on the limitations of rm-ANOVA is exemplified by the  use of  linear mixed effects models (LMEMs) by certain groups to analyze longitudinal data [@skala2010;@vishwanath2009]. Briefly, LMEMs incorporate _fixed effects_, which correspond to the levels of experimental factors in the study (e.g. the different drug regimens in a clinical trial), and _random effects_, which account for random variation within the population. When compared to the traditional rm-ANOVA, LMEMs are more flexible as they can accommodate missing observations for multiple subjects and allow different modeling strategies for the variability within each measure in every subject [@pinheiro2006]. On the other hand, LMEMs impose restrictions in the distribution of the errors  of the random effects [@gueorguieva2004].

One final assumption that is not initially evident for both rm-ANOVA and LMEMs models is that the mean response is expected to change linearly through time[@pinheiro2006]. The linearity assumption in both rm-ANOVA ane LMEMs consequently restricts the inferences from the model when the data does not follow a linear trend. In biomedical research, a particular case of this non-linear behavior in longitudinal data arises in measurements of tumor response in preclinical and clinical settings [@roblyer2011;@skala2010;@vishwanath2009]. These studies have shown that the collected signal does not follow a linear trend over time, and presents extreme variability at different time points, making the fit of and the estimations of an LMEM or rm-ANOVA model inconsistent with the observed variation. In other words, both LMEMs and rm-ANOVA explain highly-variable data using a linear trend, consequently biasing the estimates when the true response is non-linear in time. 
Additionally,  although it is possible that a _post hoc_ analysis is able to find “significant” _p-values_( _p_<0.05) it has to be reminded that the validity of such metric relies on how adequate the model is to the data. In other words, the _p-value_ requires that the model and the data have good agreement and if that is not the case, a "Type 3" error (known as "model misspecification") occurs[@dennis2019].That would be the case if for example, a model that is only able to explain linear responses is fitted to data that has a quadratic behavior. In such scenario, the _p-value_ becomes invalid[@wang2019].

As  the “frequentist” rm-ANOVA and the more flexible LMEM approaches make overly restrictive assumptions regarding the linearity of the response, there is a need for biomedical researchers to explore the use of additional statistical tools that allow the information (and not an assumed trend) to determine the fit of the model, enabling inference that is appropiate to the data.
In this regard, generalized additive models (GAMs) present an alternative approach to analyze longitudinal data. Although not commonly used in the biomedical community, these non-parametric models have been used to analyze temporal variations in geochemical and palaeoecological data  [@rose2012;@pedersen2019;@simpson2018], health-environment interactions [@yang2012] and the dynamics of government in political science [@beck1998] . There are several advantages of GAMs over LMEMs and rm-ANOVA models: GAMs can fit a more flexible class of smooth responses that enable the data to dictate the trend of the model, can easily accommodate missing observations and can model non-constant correlation between repeated measurements [@wood2017].Therefore, GAMs can provide a more flexible statistical approach to analyze non-linear biomedical longitudinal data.

 The current advances in programming languages designed for statistical analysis (specifically $\textsf{R}$), have eased the computational implementation of more complex models beyond LMEMs. In particular, $\textsf{R}$ has an extensive collection of documentation and functions to fit GAMs in the package _mgcv_ [@wood2016;@wood2017] that not only speed up the initial stages of the analysis but enable the use of advanced modeling structures (e.g. hierarchical models, confidence interval comparisons) without requiring advanced programming skills from the user. At the same time, $\textsf{R}$ has many tools that simplify data simulation, an emerging strategy used to test statistical models [@haverkamp2017]. This allows to create and  explore different alternatives for analysis without collecting information in the field, and reduces the time window between experiment design and its implementation.  

The purpose of this work is to provide biomedical researchers with a clear understanding of the theory and the practical implementation of GAMs to analyze longitudinal data using by focusing on four areas. First, the limitations of rm-ANOVA and LMEMs to analyze longitudinal data are contrasted in the cases of missing observations, assumption of linearity of response and constant correlation structures. Second, the key theoretical elements of GAMs are presented without using extensive derivations or complex mathematical notation that can obscure their understanding.  Third, simulated data that follows the trend of previously reported values [@vishwanath2009] is used to illustrate the type of non-linear longitudinal data that occurs in biomedical research to highlight the differences in inference between rm-ANOVA, LMEMs and GAMs and the implementation of the latter in such scenario. Finally, reproducibility is emphasized by providing the code to generate the data and the implementation of GAMs in $\textsf{R}$, in conjunction with a step-by-step guide to fit models of increasing complexity.  In summary, the exploration of modern statistical techniques to analyze longitudinal data may allow biomedical researchers to consider the use of GAMs instead of rm-ANOVA or LMEMs when the data does not follow a linear trend, and will also help to improve the standards for reproducibility  in biomedical research.


# Challenges presented by longitudinal studies

## The "frequentist" case for longitudinal data

The _repeated measures analysis of variance_ (rm-ANOVA) is the standard statistical analysis for longitudinal data in biomedical research, but certain assumptions are necessary for the model to be valid. From a practical view, the assumptions can be divided in three areas: a linear relationship between covariates and response, a constant correlation between measurements, and complete observations for all subjects. Each one of these assumptions is discussed below.
  
## Linear relationship 

In a biomedical longitudinal study, two or more groups of subjects (patients, mice, samples) are subject to different treatments (e.g., a "treatment" group receives a novel drug vs. a "control" group that receives a placebo), and measurements from each subject within each group are collected at specific time points. The collected response has both  _fixed_ and  _random_ components. The _fixed_ component can be understood as a constant value in the response which the researcher is interested in measuring, i.e, the effect of the novel drug in the "treatment" group. The _random_ component can be defined as "noise" caused by some inherent variability within the study. For example, if the blood concentration of the drug is measured in certain subjects in the early hours of the morning while others are measured in the afternoon, it is possible that the difference in the collection time of the measurement introduces some "noise" in the signal. As their name suggests, this "random" variability needs to be modeled as a variable rather than as a constant value.


Mathematically speaking, a rm-ANOVA model with an interaction can be written as:

\begin{equation}
y_{ijt} = \mu+ \beta_1 \times time_{t} + \beta_2 \times treatment_{j} + \beta_3 \times time_{t}\times treatment_{j}+\pi_{ij} +\varepsilon_{tij}\\ 
(\#eq:linear-model)
\end{equation}

In this model $y_{ijt}$ is the response by subject $i$, in treatment group $j$ at time $t$, which can be decomposed in a mean value $\mu$, _fixed effects_ of time ($time_t$), treatment ($treatment_j$) and their interaction $time_t*treatment_j$. A _random effect_ $\pi_{ij}$ of each subject within each group, and independent errors per subject per time per group $\varepsilon_{tij}$, that represent random variation not explained by the _fixed_ or _random_ effects. 
Suppose two treatments groups are used in the study. Then the models per group can be written as below with $j=0$ representing the first treatment group (Group A) and $j=1$ representing the second treatment group (Group B). The linear models then become:


\begin{equation}
y_{ijt} = \begin{cases}
\mu + \beta_1\times time_{t}+\varepsilon_{ijt}+\pi_{ijt}   & \mbox{if Group A}\\
\mu + \beta_1 \times time_{t} + \beta_2 \times treatment_{j} +\beta_3 \times time_{t} \times treatment_{j}+\pi_{ijt}+\varepsilon_{ijt}  & \mbox{if Group B}\\
\end{cases}
(\#eq:ANOVA-by-group)
\end{equation}

To further simplify the expression, let us substitute $\widetilde{\mu}=\mu+\beta_{2}$ and $\widetilde{\beta_{1}}=\beta_{1}+\beta_{3}$ in the equation for Group B. The equation then becomes:

\begin{equation}
y_{ijt} = \begin{cases}
\mu + \beta_1\times time_{t}+\varepsilon_{ijt}+\pi_{ijt}   & \mbox{if Group A}\\
\widetilde{\mu} + \widetilde{\beta_1} \times time_{t} +\pi_{ijt}+\varepsilon_{ijt}  & \mbox{if Group B}\\
\end{cases}
(\#eq:ANOVA-lines)
\end{equation}

Presenting the model in this manner makes clear that when treating different groups, an rm-ANOVA model is able to accommodate non-parallel lines in each case (different slopes per group). In other words, When it is stated that an rm-ANOVA model "expects" a linear relationship between the covariates and the response, this means that either presented as \@ref(eq:linear-model), \@ref(eq:ANOVA-by-group) or \@ref(eq:ANOVA-lines), an rm-ANOVA model is only able to accommodate linear patterns in the data, the maximum flexibility being that it can assign different slopes to each group. Because the construction of the model is essentially the same in LMEMs, the expected linearity holds in such models, although it is possible to fit models with different slopes at the subject-level[@pinheiro2006].  
When the data does not follow a linear trend, the fit that an rm-ANOVA or LMEM model produces does not accurately represent the trends in the data. To demonstrate this, longitudinal data where a normally distributed response of two groups of 10 subjects each is simulated, and a rm-ANOVA model  (\@ref(eq:linear-model)) is fitted to the data using the package _nlme_ (the code for the simulated data and models is available in the Appendix). 

```{r, message = FALSE,include=FALSE}
set.seed(11)
library(patchwork)
library(tidyverse)
```


```{r,include=FALSE,message=FALSE,echo=FALSE}
## Example with linear response
example <- function(n_time = 6, 
                    fun_type = "linear", 
                    error_type = "correlated") {
  
  if (!(fun_type %in% c("linear", "quadratic")))
    stop('fun_type must be either "linear", or "quadratic"')
  if (!(error_type %in% c("correlated", "independent")))
    stop('fun_type must be either "correlated", or "independent"')
  
  library(tidyverse)
  library(mvnfast)
  library(nlme)
  library(ggsci)
  
  x <- seq(0, 1, length.out = n_time)
  mu <- matrix(0, length(x), 2)
  # linear response
  if (fun_type == "linear") {
    mu[, 1] <- - (x - .5) + 0.25
    mu[, 2] <- (x - .3)
  } else {
    # nonlinear response
    
    mu[, 1] <- - 4 * (x - .5)^2 + 0.25
    mu[, 2] <- 4 * (x - .5)^2
  }
  # matplot(mu, type = 'l')
  
  y <- array(0, dim = c(length(x), 2, 10))
  errors <- array(0, dim = c(length(x), 2, 10))
  
  if (error_type == "independent") {
    ## independent errors
    for (i in 1:2) {
      for (j in 1:10) {
        errors[, i, j] <- rnorm(6, 0, 0.25)
        y[, i, j] <- mu[, i] + errors[, i, j]
      }
    }
  } else {
    for (i in 1:2) {     # number of treatments
      for (j in 1:10) {  # number of subjects
        # compound symmetry errors
        errors[, i, j] <- rmvn(1, rep(0, length(x)), 0.1 * diag(6) + 0.25 * matrix(1, 6, 6))
        y[, i, j] <- mu[, i] + errors[, i, j]
      }
    }
  }    
  
  
  ## subject random effects
  
  ## visualizing the difference between independent errors and compound symmetry
  ## why do we need to account for this -- overly confident inference
  
  
  dimnames(y) <- list(time = x, treatment = 1:2, subject = 1:10)
  dimnames(errors) <- list(time = x, treatment = 1:2, subject = 1:10)
  dimnames(mu) <- list(time = x, treatment = 1:2)
  dat <- as.data.frame.table(y, responseName = "y")
  dat_errors <- as.data.frame.table(errors, responseName = "errors")
  dat_mu <- as.data.frame.table(mu, responseName = "mu")
  dat <- left_join(dat, dat_errors, by = c("time", "treatment", "subject"))
  dat <- left_join(dat, dat_mu, by = c("time", "treatment"))
  dat$time <- as.numeric(as.character(dat$time))
  dat <- dat %>%
    mutate(subject = factor(paste(subject, treatment, sep = "-")))
  
  
  ## repeated measures ANOVA in R
  fit_lm <- lm(y ~ time + treatment + time * treatment, data = dat)
  dat$preds_lm <- predict(fit_lm)

  fit_lme <- lme(y ~ treatment + time + treatment:time,
                 data = dat,
                 random = ~ 1 | subject,
                 correlation = corCompSymm(form = ~ 1 | subject)
  )
  
  
  pred_dat <- expand.grid(
    treatment = factor(1:2), 
    time = unique(dat$time)
  )
  
  dat$y_pred <- predict(fit_lme)
  
  
  return(list(
    dat = dat,
    pred_dat = pred_dat,
    fit_lm = fit_lm,
    fit_lme = fit_lme
    
  ))
}

plot_example <- function(sim_dat) {
  library(patchwork)
  ## Plot the simulated data
  p1 <- sim_dat$dat %>%
    ggplot(aes(x = time, y = y, group = treatment, color = treatment)) +
    geom_point() +
    geom_line(aes(x = time, y = mu, color = treatment)) +
    theme_classic() +
    ggtitle("Simulated data with true response function")+
    theme(plot.title = element_text(size = 30, 
                                  face = "bold"),
        text=element_text(size=30))+
    scale_color_aaas()
  
  p2 <- sim_dat$dat %>%
    ggplot(aes(x = time, y = y, group = subject, color = treatment)) +
    geom_line(aes(size = "Subjects")) +
    # facet_wrap(~ treatment) +
    geom_line(aes(x = time, y = mu, color = treatment, size = "Simulated Truth"), lty = 1) +
    scale_size_manual(name = "Type", values=c("Subjects" = 0.5, "Simulated Truth" = 3)) +
    ggtitle("Simulated data\nIndividual responses with population mean") +
    theme(plot.title = element_text(size = 30, 
                                  face = "bold"),
        text=element_text(size=30))+
    theme_classic()+
    scale_color_aaas()
  
   p3 <- sim_dat$dat %>%
    ggplot(aes(x = time, y = errors, group = subject, color = treatment)) +
    geom_line() +
    # facet_wrap(~ treatment) +
    ggtitle("Simulated errors") +
     theme(plot.title = element_text(size = 30, 
                                  face = "bold"),
        text=element_text(size=30))+
    theme_classic()+
    scale_color_aaas()
  
  p4 <- ggplot(sim_dat$dat, aes(x = time, y = y, color = treatment)) +
    geom_point() +
    geom_line(aes(y = predict(sim_dat$fit_lme), group = subject, size = "Subjects")) +
    geom_line(data = sim_dat$pred_dat, aes(y = predict(sim_dat$fit_lme, level = 0, newdata = sim_dat$pred_dat), size = "Population")) +
    scale_size_manual(name = "Predictions", values=c("Subjects" = 0.5, "Population" = 3)) +
    theme_classic() +
    ggtitle("Fitted Model")+
    theme(plot.title = element_text(size = 30, 
                                  face = "bold"),
        text=element_text(size=30))+
    scale_color_aaas()
  
  return((p1 + p3) / (p2 + p4))
  # return((p1 + p3 + plot_layout(guides = "collect")) / (p2 + p4 + plot_layout(guides = "collect")))

}
```

```{r, linear-model, fig.width=16, fig.height=12,  echo=FALSE,message=FALSE,fig.show='hold',fig.cap="Simulated linear responses from two groups with a rm-ANOVA model fitted. Top row, simulated data, lines represent mean response. Bottom row, fitted model, thick lines represent predicted mean response"}
# linear response, correlated errors (subject effect)
txt<-18
plot_example(example(fun_type = "linear", error_type = "correlated")) +
  plot_annotation(tag_levels='A',
                  title = 'Simulated with linear function and correlated errors') &
   theme(plot.title = element_text(size = 20),text=element_text(size=txt)
)
```


It is clear from Figure \@ref(fig:linear-model) that the fit produced by the rm-ANOVA model is good as the predictions for the mean response are reasonably close to the "truth" of the simulated data.

However, consider the case when the data follows a non-linear trend, such as the simulated data in Figure \@ref(fig:quadratic-response). Here, the simulated data follows a quadratic behavior. The figure shows that changes in each group occur through the timeline, although the final mean value is the same as the initial value. Fitting an rm-ANOVA model \@ref(eq:linear-model) to this data  produces the fit that appears at the bottom right of Figure \@ref(fig:quadratic-response).

```{r quadratic-response, fig.width=16, fig.height=16,echo=FALSE,fig.cap='Simulated quadratic responses from two groups with an rm-ANOVA model fitted. Top row, simulated data, lines represent mean response. Bottom row, fitted model, thick lines represent predicted mean response', message=FALSE}
# quadratic response, correlated errors (subject effect)
txt<-18
plot_example(example(fun_type = "linear", error_type = "correlated")) +
  plot_annotation(
    tag_levels='A',
  title = 'Simulated with linear function and correlated errors') &
   theme(plot.title = element_text(size = 20),text=element_text(size=txt)
)
  # theme(plot.title = element_text(size = 20))
```

In this case, compare the predictions to the simulated data and it is noticeable that the model is not capturing the changes within each group throughout the timeline. This highlights the limitation of rm-ANOVA and LMEMs with longitudinal non-linear data, where the model is only flexible enough to allow different slopes per group, but is unable to follow any trend that is highly variable.

## Covariance in rm-ANOVA and LMEMs

In a longitudinal study there is an expected _variance_ between repeated measurements on the same subject, and because repeated measures occur in the subjects within each group, there is a _covariance_ between  measurements at each time point within each group. The _covariance matrix_ (also known as the variance-covariance matrix) is a matrix that captures the variation between and within subjects in a longitudinal study[@wolfinger1996] (For an in-depth analysis of the covariance matrix see [@west2014;@weiss2005]). 

In the case of an rm-ANOVA analysis, it is typically assumed that the covariance matrix has a specific construction known as _compound symmetry_ (also known as "sphericity" or "circularity"). Under this assumption, the between-subject variance and within-subject correlation  are constant across time [@weiss2005;@geisser1958;@huynh1976]. However, it has been shown that this condition is frequently unjustified because the correlation between measurements tends to change over time [@maxwell2017]; and it is higher between consecutive measurements [@gueorguieva2004;@ugrinowitsch2004]. A visual representation of the 
Although corrections can be made (such as Huyhn-Feldt or Greenhouse-Geisser) the effectiveness of each correction is limited because it depends on the size of the sample,the number of repeated measurements[@haverkamp2017], and they are not robust if the group sizes are unbalanced [@keselman2001]. In other words, if the data does not present constant correlation between repeated measurements, the assumptions required for an rm-ANOVA model are not met and the use of corrections may still not provide a reasonable adjustment that makes the model valid.


In the case of LMEMs, one key advantage over rm-ANOVA is that they allow different structures for the variance-covariance matrix including exponential, autoregressive of order 1, rational quadratic and others [@pinheiro2006]. Nevertheless, the analysis required to determine an appropriate variance-covariance structure for the data can be a long process by itself. Overall, the spherical assumption for rm-ANOVA may not capture the natural variations of the correlation in the data, and can bias the inferences from the analysis. 


## Missing observations

Missing observations are an issue that arises frequently in longitudinal studies. In biomedical research, this situation can be caused by reasons beyond the control of the investigator [@molenberghs2004]. Dropout from patients, attrition or injury in animals are among the reasons for missing observations. Statistically, missing information can be classified as _missing at random_ (MAR), _missing completely at random_ (MCAR), and _missing not at random_ (MNAR) [@weiss2005].  In a MAR scenario, the pattern of the missing information is related to some variable in the data, but it is not related to the variable of interest [@scheffer2002]. If the data are MCAR, this means that the missigness is completely unrelated to the collected information [@potthoff2006], and in the case of MNAR the missing values are dependent on their value. An rm-ANOVA model assumes complete observations for all subjects, and therefore subjects with one or more missing observations are excluded from the analysis. This is inconvenient because the remaining subjects might not accurately represent the population, and statistical power is affected by this reduction in sample size [@ma2012].

In the case of LMEMs, inferences from the model are valid when missing observations in the data exist that are MAR or MCAR [@west2014]. The pattern of missing observations can be considered MAR if the missing observations are not related any of the other variables measured in the study [@maxwell2017]. For example, if attrition occurs in all mice that had lower weights at the beginning of a chemotherapy response study, the missing data can be considered MAR because  the missigness is unrelated to other variables of interest.

This section has presented the assumptions of rm-ANOVA to analyze longitudinal information and  its differences when compared to LMEMs  regarding to missing data and the modeling of the covariance matrix. Of notice, LMEMs offer a more robust and flexible approach than rm-ANOVA and if the data follows a linear trend, they provide an excellent choice to derive inferences from a repeated measures study. However, when the data presents high variability, LMEMs fail to capture the non-linear trend of the data. To analyze such type of data, we present generalized additive models (GAMs) as an alternative in the following section.

# GAMs as a special case of Generalized Linear Models

## GAMs and Basis Functions

A GAM is a special case of the Generalized Linear Model (GLM), a framework that allows for response distributions that do not follow a normal distribution [@wood2017;@hastie1987].  Following the notation by Simpson [@simpson2018] A GAM model can be represented as:


\begin{equation}
  y_{ijt}=\beta_0+f(x_t\mid \beta)+\varepsilon_{ijt}
  (\#eq:GAM)
\end{equation}

Where $y_{ijt}$ is the response at time $t$, $\beta_0$ is the expected value at time 0, the change of $y_{ijt}$ over time is represented by the function $f(x_t\mid \beta)$ and $\varepsilon_{ijt}$ represents the residuals.

In contrast to rm-ANOVA or LMEMs, GAMs use _smooth functions_  to model the relationship between the covariates and the response. This approach is more advantageous as it does not restrict the model to a linear relationship. One possible function for $f(x_t\mid \beta)$ that allows for non-linear responses is a polynomial, but a major limitation is that they create a "global" fit as they assume that the same relationship exists everywhere, which can cause problems with the fit [@beck1998].In particular, as $t$ goes to $\pm \infty$, $f(x_t \mid \beta)$ goes to $\pm \infty$ which is almost always unrealistic.


The model specification for GAMs requires that the _smooth functions_ are represented in a parametric way that fit within the GLM framework, and this step is achieved by using _basis functions_ to represent them. A _basis_ is a set of functions that capture the space where the smooths that approximate $f(x_t \mid \beta)$ exist [@simpson2018]. For the linear model in \@ref(eq:linear-model), the basis coefficients are $\beta_1$, $\beta_2$ and $\beta_3$ and the basis vectors are $time_t$, $treatment_j$ and $time_t \times treatment_j$. The basis function then, is the combination of basis coefficients and basis vectors that map the possible relationship between the covariates and the response [@hefley2017], which in the case of \@ref(eq:linear-model) is restricted to a linear behavior.  In the case of \@ref(eq:GAM), the basis function is $f(x_t\mid \beta)$, which means that the model allows relationships beyond linear for the covariates.


In GAMs the _basis functions_  are represented over  evenly spaced ranges of the covariates known as _knots_. A commonly used _basis function_ is a cubic spline, which is a smooth curve constructed from cubic polynomials joined together at the knot locations[@wood2017;@simpson2018]. Cubic splines have a long history in solving non-parametric statistical problems and are often a default choice to fit GAMs as they are a simple, flexible and powerful option to obtain visual smoothness [@wegman1983]. Therefore, GAMs overcome the limitation that occurs in LMEMs and rm-ANOVA when the data is non linear, such as Figure \@ref(fig:quadratic-response). Regarding longitudinal data, Pedersen et al [@pedersen2019] demonstrated the capabilities of GAMs in this area using ecological data.

The use of GAMs to analyze biomedical longitudinal data, and the impact of missing observations in the fit of the model will be examined in detail in the following section using simulation.



***



# References
