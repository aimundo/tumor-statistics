---
title: '**The statistical analysis of non-linear longitudinal data in biomedical research**'
author:
- Ariel Mundo^[Department of Biomedical Engineering, University of Arkansas, Fayetteville]
- Timothy J. Muldoon^[Department of Biomedical Engineering, University of Arkansas,
  Fayetteville]
- John R. Tipton^[Department of Mathematical Sciences, University of Arkansas, Fayetteville]
subtitle: _Beyond repeated measures ANOVA and Linear Mixed Models_
bibliography: refs.bib
csl: ieee.csl
css: style.css
fontfamily: Arial
  output:
  
  
  bookdown::html_document2: default
  bookdown::pdf_document2:
    keep_tex: yes
    fig_caption: yes
  bookdown::word_document2: default
  link-citations: true
---

# Background

A longitudinal study is defined as that which is designed to repeatedly measure a variable of interest in a group (or groups) of subjects. In biomedical research, this type of study is employed when the intention is to observe the evolution of the effect of a certain treatment across time, rather than analyzing it at a single time point (a cross-sectional study). Clinical studies of breast and neck cancer have used a longitudinal approach [@sio2016;@kamstra2015]. In the first case, weekly measurements of skin toxicities in  patients with radiation-induced dermatitis were taken for up to 8 weeks; whereas in the latter mouth opening was assessed at 6,12, 18, 24 and 36 months after radiotherapy (RT). Longitudinal studies have beenn also used to analyze tumor response [@roblyer2011;@tank2020;@pavlov2018;@demidov2018], antibody expression [@ritter2001;@roth2017], and cell metabolism [@jones2018;@skala2010]. 


Traditionally, a “frequentist” approach is used in biomedical research to derive inferences from a longitudinal study. Such statistical view derives its name from the fact that it regards probability as a limiting frequency[@wagenmakers2008] and its application is based on a null hypothesis test using the _analysis of variance over repeated measures_ (repeated measures ANOVA or rm-ANOVA). This methodology makes two key assumptions regarding longitudinal data: constant correlation exists across same-subject measurements, and observations from each subject are obtained at all time points through the study (a condition also known as complete observations) [@gueorguieva2004;@schober2018].

However, constant correlation is frequently unjustified as its value tends to diminish between measures when the time interval between them increases [@ugrinowitsch2004]. 
In the case of the number of observations during the study, different situations can arise that prevent the collection of complete measurements for all subjects:  in a clinical trial voluntary withdrawal from one or multiple patients can occur, whereas attrition in animals due to injury or weight loss can occur in preclinical experiments, and it is even possible that unexpected complications with equipment or supplies arise, preventing the researcher from collecting measurements at a certain time point and therefore violating the _complete observations_ assumption of rm-ANOVA. 

When incomplete observations occur, an rm-ANOVA requires the exclusion of all subjects with missing observations from the analysis. This can result in increased costs if the desired statistical power is not met with the remaining observations, as it would be necessary to enroll more subjects. At the same time, if the excluded observations contain insightful information that is not used, their elimination from the analysis may limit the demonstration of significant differences between groups. Additionally, rm-ANOVA uses a _post hoc_ analysis to assess differences between the measured response in different groups. A _post hoc_ analysis is based on multiple repeated comparisons to estimate a _p-value_, a metric that is widely used as a measure of significance. Because the _p-value_ is highly variable, multiple comparisons can inflate the false positivity rate [@liu2010;@halsey2015], consequently biasing the conclusions of the study.

During the last decade, the biomedical community has started to recognize the limitations of rm-ANOVA in the analysis of longitudinal information. This is exemplified by the  use of  linear mixed effects models (LMEMs)  in certain groups to analyze longitudinal data [@skala2010;@vishwanath2009]. Briefly, these models incorporate _fixed effects_, which correspond to the levels of experimental factors in the study (e.g. the different drug regimens in a clinical trial), and _random effects_, which account for random variation within the population. These models are more flexible than the traditional rm-ANOVA as they can accommodate missing observations for multiple subjects and allow different modeling strategies for the variability within each measure in every subject [@pinheiro2006]. On the other hand, they impose restrictions in the distribution of the errors  of the random effects [@gueorguieva2004].

One final assumption that is not initially evident for  both rm-ANOVA and LMEMs models pertains the relationship between the observed effect and the progression of the timeline of the study, which is expected to be linear [@pinheiro2006]. This common assumption in both rm-ANOVA ane LMEMs consequently restricts the inferences from the model when the data does not follow a linear trend. In biomedical research, a special case of this behavior in longitudinal data arises in measurements of tumor response in preclinical and clinical settings [@roblyer2011;@skala2010;@vishwanath2009]. These studies have shown that the collected signal does not follow a linear trend over time, and presents extreme variability at different time points, making the fit of and the estimations of an LMEM or rm-ANOVA model inconsistent with the observed variation. In other words, the in both cases the model tries to explain highly-variable data using a linear trend, consequently biasing the estimates. Additionally,  although it is possible that a _post hoc_ analysis is able to find “significant” _p-values_( _p_<0.05) by using multiple comparisons between the model terms, the use of this estimator is limited in such scenario because it is intended to work in models that have a reasonable agreement with the data.

As  the “frequentist” rm-ANOVA and the more advanced LMEM approach are both limited in the analysis of non-linear longitudinal information, there is a need for biomedical researchers to explore the use of additional statistical tools that allow the information (and not an assumed trend) to determine the fit of the model, while enabling inferences that are both adequate and consistent from a statistical perspective.
In this regard, Generalized Additive Models (GAMs) present an alternative approach to analyze longitudinal data. Although not commonly used in the biomedical community, these non-parametric models have been used to analyze temporal variations in geochemical and palaeoecological data  [@rose2012;@pedersen2019;@simpson2018], health-environment interactions [@yang2012] and political science [@beck1998] . There are several advantages of GAMs over LMEMs and rm-ANOVA models: They have an extensively developed theoretical background, enable the data to dictate the trend of the model, can accommodate missing observations and do not require constant correlation between repeated measurements [@wood2017].Therefore, GAMs can provide a more suitable statistical method to analyze non-linear biomedical longitudinal data.

 The current advances in programming languages designed for statistical analysis (specifically $\textsf{R}$), have eased the computational implementation of more complex models beyond LMEMs. In particular, $\textsf{R}$ has an extensive collection of documentation and functions to fit GAMs in the package _mgcv_ [@wood2011;@wood2016;@wood2017] that not only speed up the initial stages of the analysis but enable the use of advanced modeling structures (e.g. hierarchical models, confidence interval comparisons) without requiring advanced programming skills from the user. This programming language is also able to simulate data, an emerging strategy used to test statistical models [@haverkamp2017]. This allows to create and  explore different alternatives for analysis without collecting information in the field, and reduces the time window between experiment design and its implementation.  

Therefore, the purpose of this study is to provide biomedical researchers with a clear understanding of the theory and the practical implementation of GAMs to analyze longitudinal data using by focusing on four areas. First, the limitations of rm-ANOVA and LMEMs to analyze longitudinal data are contrasted in the cases of missing observations, assumption of linearity and correlation structures. Secondly, the key theoretical elements of GAMS are presented without using extensive derivations or complex mathematical notation that can obscure their understanding.  Thirdly, simulated data that follows the trend of previously reported values [@vishwanath2009] is used to illustrate the type of non-linear longitudinal data that occurs in biomedical research and the implementation of GAMs in such scenario. Finally, reproducibility is emphasized by providing the code to generate the data and the implementation of GAMs in $\textsf{R}$, in conjunction with a step-by-step guide to fit models of increasing complexity.  
In summary, the exploration of modern statistical techniques to analyze longitudinal data may allow biomedical researchers to consider the use of GAMs instead of rm-ANOVA or LMEMs when the data does not follow a linear trend, and will also help to improve the standards for reproducibility  in biomedical research.


# Challenges presented by longitudinal studies

## The "frequentist" case for longitudinal data

The _repeated measures analysis of variance_ (rm-ANOVA) is the standard statistical analysis for longitudinal data in biomedical research, but certain assumptions are necessary to make the model valid. From a practical view, they can be divided in three areas: linear relationship between covariates and response, constant correlation between measurements, and complete observations for all subjects. Each one of these assumptions is discussed below.
  
## Linear relationship 

In a biomedical longitudinal study, two or more groups of subjects (patients, mice, samples) are subject to a different treatments (e.g. a "treatment" group receives a novel drug vs. a "control" group that receives a placebo), and measurements from each subject within each group are collected at specific time points. The collected response has both  _fixed_ and  _random_ components. The _fixed_ component can be understood as a constant value in the response which the researcher is interested in measuring, i.e, the effect of the novel drug in the "treatment" group.The _random_ component can be defined as "noise" caused by some inherent variability within the study. For example, if the blood concentration of the drug is measured in certain subjects in the early hours of the morning while others are measured in the afternoon, it is possible that the difference in the collection time of the measurement introduces some "noise" in the signal. As their name suggests, this "random" variability needs to be modeled as a variable rather than as a constant value.


Mathematically speaking, a rm-ANOVA model can be written as:


\begin{equation}
  y_{i} = \mu+ \beta_1* time_{t} + \beta_2* treatment_{j} + \beta_3* time_{t} *                                    treatment_{j}+\pi_{ij} +\epsilon_{tij}\\ 
  (\#eq:linear-model)
\end{equation}


In this model $y_i$ is the response by subject $i$, which can be decomposed in a mean value $\mu$, _fixed effects_ of time ($time_t$), treatment ($time_j$) and their interaction; a _random effect_ $\pi_{ij}$ of each subject within each group, and errors per subject per time per group $\epsilon_{tij}$. 
Suppose two treatments groups are used in the study. Models per group can be generated if 0 is used for the first treatment group (Group A) and 1 for the second treatment group (Group B). The linear models then become:


\begin{equation}
  y = \begin{cases}
  \mu + \beta_1*time_{t}+\epsilon_{ti}   & \mbox{if Group A}\\
  \mu + \beta_1 * time_{t} + \beta_2 * treatment_{j} +\beta_3* time_{t} * treatment_{j}+\pi_{i}+\epsilon_{tij}  & \mbox{if Group B}\\
  \end{cases}
  (\#eq:ANOVA-by-group)
\end{equation}



Furthermore, \@ref(eq:ANOVA-by-group) can be re-written under the generalized linear model  (GLM) framework [@nelder1972]. The main difference in this case is that a GLM model  uses a _linking function_ to connect the observed value to the linear model. 

Thus, 

\begin{equation}
  y =\mu +\boldsymbol{\beta}_1* f(time) + \beta_2* treatment+\boldsymbol{\beta}_3*             f(time)*treatment \\

  y = \begin{cases}
  \mu +\boldsymbol{\beta}_1* f(time)  & \mbox{if Group A} \\
  \mu +\boldsymbol{\beta}_1* f(time)  + \beta_2* treatment + \boldsymbol{\beta}_3* f(time) *     treatment   & \mbox{if Group B}\\
  \end{cases}
  (\#eq:GLM)
\end{equation}


The linear model in \@ref(eq:linear-model) is a GLM where $f(time) = time$ with the identity function $f(x) = x$.

Regardless of its presentation, \@ref(eq:linear-model), \@ref(eq:ANOVA-by-group) and \@ref(eq:GLM) show that the model expects a linear relationship between the covariates and the response. When the data does not follow a linear trend, the fit that the model produces does not accurately represent the changes in the data. To exemplify this consider simulated longitudinal data where the a normally distributed response of two groups of 10 subjects each is obtained, and an rm-ANOVA model such as \@ref(eq:linear-model) is fitted to the data (code for the simulated data is available in the Appendix).  

```{r, message = FALSE,include=FALSE}
set.seed(11)
library(patchwork)
library(tidyverse)
```


```{r,include=FALSE,message=FALSE,echo=FALSE}
## Example with linear response
example <- function(n_time = 6, 
                    fun_type = "linear", 
                    error_type = "correlated") {
  
  if (!(fun_type %in% c("linear", "quadratic")))
    stop('fun_type must be either "linear", or "quadratic"')
  if (!(error_type %in% c("correlated", "independent")))
    stop('fun_type must be either "correlated", or "independent"')
  
  library(tidyverse)
  library(mvnfast)
  library(nlme)
  
  x <- seq(0, 1, length.out = n_time)
  mu <- matrix(0, length(x), 2)
  # linear response
  if (fun_type == "linear") {
    mu[, 1] <- - (x - .5) + 0.25
    mu[, 2] <- (x - .3)
  } else {
    # nonlinear response
    mu[, 1] <- - 4 * (x - .5)^2 + 0.25
    mu[, 2] <- 4 * (x - .5)^2
  }
  # matplot(mu, type = 'l')
  
  y <- array(0, dim = c(length(x), 2, 10))
  errors <- array(0, dim = c(length(x), 2, 10))
  
  if (error_type == "independent") {
    ## independent errors
    for (i in 1:2) {
      for (j in 1:10) {
        errors[, i, j] <- rnorm(6, 0, 0.25)
        y[, i, j] <- mu[, i] + errors[, i, j]
      }
    }
  } else {
    for (i in 1:2) {     # number of treatments
      for (j in 1:10) {  # number of subjects
        # compound symmetry errors
        errors[, i, j] <- rmvn(1, rep(0, length(x)), 0.1 * diag(6) + 0.25 * matrix(1, 6, 6))
        y[, i, j] <- mu[, i] + errors[, i, j]
      }
    }
  }    
  
  
  ## subject random effects
  
  ## visualizing the difference between independent errors and compound symmetry
  ## why do we need to account for this -- overly confident inference
  
  
  dimnames(y) <- list(time = x, treatment = 1:2, subject = 1:10)
  dimnames(errors) <- list(time = x, treatment = 1:2, subject = 1:10)
  dimnames(mu) <- list(time = x, treatment = 1:2)
  dat <- as.data.frame.table(y, responseName = "y")
  dat_errors <- as.data.frame.table(errors, responseName = "errors")
  dat_mu <- as.data.frame.table(mu, responseName = "mu")
  dat <- left_join(dat, dat_errors, by = c("time", "treatment", "subject"))
  dat <- left_join(dat, dat_mu, by = c("time", "treatment"))
  dat$time <- as.numeric(as.character(dat$time))
  dat <- dat %>%
    mutate(subject = factor(paste(subject, treatment, sep = "-")))
  
  
  ## repeated measures ANOVA in R
  fit_lm <- lm(y ~ time + treatment + time * treatment, data = dat)
  dat$preds_lm <- predict(fit_lm)

  fit_lme <- lme(y ~ treatment + time + treatment:time,
                 data = dat,
                 random = ~ 1 | subject,
                 correlation = corCompSymm(form = ~ 1 | subject)
  )
  
  
  pred_dat <- expand.grid(
    treatment = factor(1:2), 
    time = unique(dat$time)
  )
  
  dat$y_pred <- predict(fit_lme)
  
  
  return(list(
    dat = dat,
    pred_dat = pred_dat,
    fit_lm = fit_lm,
    fit_lme = fit_lme
    
  ))
}


plot_example <- function(sim_dat) {
  library(patchwork)
  ## Plot the simulated data
  p1 <- sim_dat$dat %>%
    ggplot(aes(x = time, y = y, group = treatment, color = treatment)) +
    geom_point() +
    geom_line(aes(x = time, y = mu, color = treatment)) +
    theme_bw(base_size = 12) +
    ggtitle("Simulated data with true response function")
  
  p2 <- sim_dat$dat %>%
    ggplot(aes(x = time, y = y, group = subject, color = treatment)) +
    geom_line(aes(size = "Subjects")) +
    # facet_wrap(~ treatment) +
    geom_line(aes(x = time, y = mu, color = treatment, size = "Simulated Truth"), lty = 1) +
    scale_size_manual(name = "Type", values=c("Subjects" = 0.5, "Simulated Truth" = 3)) +
    ggtitle("Simulated data\nIndividual responses with population mean") +
    theme_bw(base_size = 12)
  
   p3 <- sim_dat$dat %>%
    ggplot(aes(x = time, y = errors, group = subject, color = treatment)) +
    geom_line() +
    # facet_wrap(~ treatment) +
    ggtitle("Simulated errors") +
    theme_bw(base_size = 12)
  
  p4 <- ggplot(sim_dat$dat, aes(x = time, y = y, color = treatment)) +
    geom_point() +
    geom_line(aes(y = predict(sim_dat$fit_lme), group = subject, size = "Subjects")) +
    geom_line(data = sim_dat$pred_dat, aes(y = predict(sim_dat$fit_lme, level = 0, newdata = sim_dat$pred_dat), size = "Population")) +
    scale_size_manual(name = "Predictions", values=c("Subjects" = 0.5, "Population" = 3)) +
    theme_bw(base_size = 12) +
    ggtitle("Fitted Model")
  return((p1 + p3) / (p2 + p4))
  # return((p1 + p3 + plot_layout(guides = "collect")) / (p2 + p4 + plot_layout(guides = "collect")))

}
```

```{r, fig.width=16, fig.height=9, out.width="100%", echo=FALSE,message=FALSE}
# linear response, correlated errors (subject effect)
plot_example(example(fun_type = "linear", error_type = "correlated")) +
  plot_annotation(
  title = 'Simulated with linear function and correlated errors',
    theme = theme(plot.title = element_text(size = 20))
) #& 
  # theme(plot.title = element_text(size = 20))
```
It is clear from this graph that

```{r, fig.width=16, fig.height=9, out.width="100%"}
# quadratic response, correlated errors (subject effect)
plot_example(example(fun_type = "quadratic", error_type = "correlated")) +
  plot_annotation(
  title = 'Simulated with quadratic function and correlated errors',
    theme = theme(plot.title = element_text(size = 20))
) #& 
  # theme(plot.title = element_text(size = 20))
```


## Covariance in rm-ANOVA and LMEMs

In a longitudinal study there is an expected _variance_ between repeated measurements on the same subject, and because repeated measures occur in the subjects within each group, there is a _covariance_ between  measurements at each time point within each group. The _covariance matrix_ (also known as the variance-covariance matrix) is a matrix that captures the variation between and within subjects in a longitudinal study[@wolfinger1996] (For an in-depth analysis of the covariance matrix see [@west2014;@weiss2005]). 

In the case of an rm-ANOVA analysis, it is assumed that the covariance matrix has a specific construction known as _compound symmetry_ (also known as "sphericity" or "circularity"). Under this assumption, the between-subject variance and within-subject correlation  are constant across time [@weiss2005;@geisser1958;@huynh1976]. However, it has been shown that this condition is frequently unjustified because the correlation between measurements tends to change over time [@maxwell2017]; and it is higher between consecutive measurements [@gueorguieva2004;@ugrinowitsch2004]. Although corrections can be made (such as Huyhn-Feldt or Greenhouse-Geisser) the effectiveness of each correction is limited because it depends on the size of the sample,the number of repeated measurements[@haverkamp2017], and they are not robust if the group sizes are unbalanced [@keselman2001]. In other words, if the data does not present constant correlation between repeated measurements, the assumptions required for an rm-ANOVA model are not met and the use of corrections may still not provide a reasonable adjustment that makes the model valid.


In the case of LMEMs, one key advantage over rm-ANOVA is that they allow different structures for the variance-covariance matrix including exponential, autoregressive of order 1, rational quadratic and others [@pinheiro2006]. Nevertheless, the analysis required to determine an appropriate variance-covariance structure for the data can be a long process by itself. Overall, the spherical assumption for rm-ANOVA may not capture the natural variations of the correlation in the data, and can bias the inferences from the analysis. 


## Missing observations

Missing observations are an issue that arises frequently in longitudinal studies. In biomedical research, this situation can be caused by reasons beyond the control of the investigator [molenberghs2004].Dropout from patients, or attrition or injury in animals are among the reasons for missing observations. Statistically, missing information can be classified as _missing at random_ (MAR), _missing completely at random_ (MCAR), and _missing not at random_ (MNAR) [@weiss2005].  In a MAR scenario, the pattern of the missing information is related to some variable in the data, but it is not related to the variable of interest [@scheffer2002]. If the data are MCAR, this means that the missigness is completely unrelated to the collected information [@potthoff2006], and in the case of MNAR the missing values are dependent on their value. An rm-ANOVA model assumes complete observations for all subjects, and therefore subjects with one or more missing observations are excluded from the analysis. This is inconvenient because the remaining subjects might not accurately represent the population, and statistical power is affected by this reduction in sample size [@ma2012].

In the case of LMEMs, inferences from the model are valid when missing observations in the data exist that are MAR [@west2014]. The pattern of missing observations can be considered MAR if the missing observations are not related any of the other variables measured in the study [@maxwell2017]. For example, if attrition occurs in all mice that had lower weights at the beginning of a chemotherapy response study, the missing data can be considered MAR because  the missigness is unrelated to other variables of interest.

_What limitations LMEMs have regarding missing observations?_

This section has presented the assumptions of rm-ANOVA to analyze longitudinal information and  its differences when compared to LMEMs  regarding to missing data and the modeling of the covariance matrix. Of notice, LMEMs offer a more robust and flexible approach than rm-ANOVA and if the data follows a linear trend, they provide an excellent choice to derive inferences from a repeated measures study. However, when the data presents high variability, LMEMs fail to capture the non-linear trend of the data. To analyze such type of data, we present generalized additive models (GAMs) as an alternative in the following section.

# GAMs as a special case of Generalized Linear Models

A GAM is a special case of the Generalized Linear Model (GLM), a framework that allows for response distributions that do not follow a normal distribution [@wood2017;@hastie1987].  Following the notation by Simpson [@simpson2018] A GAM model can be represented as:


\begin{equation}
  y_{t}=\beta_0+f(x_t)+\epsilon_t  
  (\#eq:GAM)
\end{equation}

Where $y_t$ is the response at time $t$, $\beta_0$ is the expected value at time 0, the change of $y$ over time is represented by the function $f(x_t)$ and $\epsilon_t$ represents the residuals.

In contrast to rm-ANOVA or LMEMs, GAMs use _smooth functions_  to model the relationship between the covariates and the response. This approach is more advantageous than that of a LMEMs or rm-ANOVA as it does not restrict the model to a linear relationship. One possible function for $f(x_t)$  is a polynomial, but a major limitation is that they create a "global" fit as they assume that the same relationship exists everywhere, which can cause problems with the fit [@beck1998].

To overcome this limitation, the smooth functions in GAMs  are represented using _basis functions_  over  evenly spaced ranges of the covariates known as _knots_. The _basis function_ used is a cubic spline, which is a smooth curve constructed from cubic polynomials joined together[@wood2017;@simpson2018]. Cubic splines have a long history in their use to solve non-parametric statistical problems and are the default choice to fit GAMs as they are the simplest option to obtain visual smoothness [@wegman1983].





***



# References
